{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNasD+F8qfkTSVQZfNAIepg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tymothy6/colab/blob/main/Real_time_(ish)_Whisper_transcription_%5BGradio%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install dependencies"
      ],
      "metadata": {
        "id": "ockUswiTYRky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OpenAI Python library is used to make API calls to Whisper and GPT. Gradio is used to create the user interface for audio input and transcription outputs."
      ],
      "metadata": {
        "id": "q-U4U4LysJ3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQCDfZSYzfoF"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Define Whisper and GPT API calls and run the Gradio app:"
      ],
      "metadata": {
        "id": "sqerAIQ-YY6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "import openai\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "Nrg9FS7O0lMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper API call\n",
        "def transcribe(audio):\n",
        "  with open(audio, \"rb\") as audio_file:\n",
        "    try:\n",
        "      response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "      transcription = response['text']\n",
        "      if not transcription:\n",
        "        return \"Error transcribing audio.\", \"\"\n",
        "\n",
        "      return transcription, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {e}\")\n",
        "      print(f\"API Response: {response.json() if response else 'No response'}\")\n",
        "      return f\"An error occurred: {e}\", \"\"\n",
        "\n",
        "\n",
        "# Post-processing with GPT\n",
        "def generate_correction(transcription):\n",
        "  system_prompt = (\"You are a helpful assistant. Your task is to correct spelling, grammar, and appropriate punctuation in the transcribed text.\")\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-4\",\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": system_prompt\n",
        "          },\n",
        "          {\n",
        "              \"role\":\"user\",\n",
        "              \"content\": transcription\n",
        "          }\n",
        "      ]\n",
        "  )\n",
        "  return response['choices'][0]['message']['content']\n",
        "\n",
        "def post_process(transcription):\n",
        "  return generate_correction(transcription)\n",
        "\n",
        "def main_function(audio, option):\n",
        "    transcription, error = transcribe(audio)\n",
        "    if error:\n",
        "      return error, \"\"\n",
        "    if option == \"Speech-to-text\":\n",
        "        return transcription, \"\"\n",
        "    elif option == \"Speech-to-text with post-processing\":\n",
        "        processed_text = post_process(transcription)\n",
        "        return transcription, processed_text\n",
        "\n",
        "# Gradio app\n",
        "\n",
        "audio_input = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Speech input\")\n",
        "option_dropdown = gr.Dropdown(choices=[\"Speech-to-text\", \"Speech-to-text with post-processing\"], label=\"Options\")\n",
        "textbox_output = gr.Textbox(label=\"Text\")\n",
        "textbox_processed_output = gr.Textbox(label=\"Processed text\")\n",
        "\n",
        "def combined_interface(audio):\n",
        "  # Retrieve the transcription\n",
        "  transcription = transcribe(audio)\n",
        "  # Post-process the transcription\n",
        "  processed_transcription = post_process(transcription)\n",
        "  return transcription, processed_transcription\n",
        "\n",
        "whisper = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=[audio_input, option_dropdown],\n",
        "    outputs=[textbox_output, textbox_processed_output],\n",
        "    title=\"Whisper API\",\n",
        "    description=\"This is a demo of Whisper live transcription with GPT post-processing using the OpenAI API.\",\n",
        "    theme=\"dark\",\n",
        "    allow_flagging=\"never\", # disables the Flag feature\n",
        "    live=False  # when this is true the function is called in real-time, without needing to press the button\n",
        ")\n",
        "\n",
        "whisper.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "vDsl2Az5YYLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
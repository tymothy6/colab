{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tymothy6/colab/blob/main/Real_time_(ish)_Whisper_transcription_%5BGradio%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ockUswiTYRky"
      },
      "source": [
        "### 1. Install dependencies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q-U4U4LysJ3H"
      },
      "source": [
        "The OpenAI Python library is used to make API calls to Whisper and GPT. Gradio is used to create the user interface for audio input and transcription outputs. The PyDub library is used to chunk the recorded audio into segments to mimic real-time transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQCDfZSYzfoF"
      },
      "outputs": [],
      "source": [
        "%pip install gradio\n",
        "%pip install openai\n",
        "%pip install pydub"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sqerAIQ-YY6h"
      },
      "source": [
        "### 2. Define Whisper and GPT API calls and run the Gradio app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrg9FS7O0lMe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "import openai\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDsl2Az5YYLi"
      },
      "outputs": [],
      "source": [
        "# Whisper API call\n",
        "def transcribe(audio):\n",
        "  with open(audio, \"rb\") as audio_file:\n",
        "    try:\n",
        "      response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "      transcription = response['text']\n",
        "      if not transcription:\n",
        "        return \"Error transcribing audio.\", \"\"\n",
        "\n",
        "      return transcription, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {e}\")\n",
        "      print(f\"API Response: {response.json() if response else 'No response'}\")\n",
        "      return f\"An error occurred: {e}\", \"\"\n",
        "\n",
        "\n",
        "# Post-processing with GPT4\n",
        "def generate_correction(transcription):\n",
        "  system_prompt = (\"You are a helpful assistant. Your task is to correct spelling, grammar, and appropriate punctuation in the transcribed text.\")\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-4\",\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": system_prompt\n",
        "          },\n",
        "          {\n",
        "              \"role\":\"user\",\n",
        "              \"content\": transcription\n",
        "          }\n",
        "      ]\n",
        "  )\n",
        "  return response['choices'][0]['message']['content']\n",
        "\n",
        "def post_process(transcription):\n",
        "  return generate_correction(transcription)\n",
        "\n",
        "def main_function(audio, option):\n",
        "    transcription, error = transcribe(audio)\n",
        "    if error:\n",
        "      return error, \"\"\n",
        "    if option == \"Speech-to-text\":\n",
        "        return transcription, \"\"\n",
        "    elif option == \"Speech-to-text with post-processing\":\n",
        "        processed_text = post_process(transcription)\n",
        "        return transcription, processed_text\n",
        "\n",
        "# Gradio app\n",
        "\n",
        "audio_input = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Speech input\")\n",
        "option_dropdown = gr.Dropdown(choices=[\"Speech-to-text\", \"Speech-to-text with post-processing\"], label=\"Options\", value=\"Speech-to-text\")\n",
        "textbox_output = gr.Textbox(label=\"Text\")\n",
        "textbox_processed_output = gr.Textbox(label=\"Processed text\")\n",
        "\n",
        "\n",
        "whisper = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=[audio_input, option_dropdown],\n",
        "    outputs=[textbox_output, textbox_processed_output],\n",
        "    title=\"Whisper API\",\n",
        "    description=\"This is a demo of Whisper live transcription with GPT post-processing using the OpenAI API.\",\n",
        "    allow_flagging=\"never\", # disables the Flag feature\n",
        "    live=False  # when this is true the function is called in real-time, without needing to press the button\n",
        ")\n",
        "\n",
        "whisper.launch(debug=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Real-time transcription using Gradio state variables\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the `live` parameter of `gr.Interface` combined with state to mimic real-time transcription with the Whisper API. The idea is to periodically process audio chunks and accumulate the transcribed text without disrupting the user experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "import openai\n",
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def transcribe_from_point(audio, last_point):\n",
        "    # Error handling when audio is empty\n",
        "    if not audio:\n",
        "        print(\"No audio file provided.\")\n",
        "        return \"\", last_point\n",
        "    # Load the audio file from the last point\n",
        "    audio_segment = AudioSegment.from_wav(audio)[last_point:]\n",
        "    \n",
        "    # If the new chunk is empty or too small, return without transcribing\n",
        "    if len(audio_segment) < 1000:  # less than 1 second\n",
        "        return \"\", last_point\n",
        "\n",
        "    # Save the new chunk to a temporary file for transcription\n",
        "    chunk_filename = \"temp_chunk.wav\"\n",
        "    audio_segment.export(chunk_filename, format=\"wav\")\n",
        "\n",
        "    # Transcribe this chunk\n",
        "    with open(chunk_filename, \"rb\") as audio_file:\n",
        "        try:\n",
        "            response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "            os.remove(chunk_filename)  # delete the temporary file\n",
        "            if response['text']:\n",
        "                return response['text'], last_point + len(audio_segment)\n",
        "            else:\n",
        "                return \"\", last_point\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            os.remove(chunk_filename)  # delete the temporary file\n",
        "            return f\"An error occurred: {e}\", last_point\n",
        "        \n",
        "# Post-processing with GPT4\n",
        "def generate_correction(transcription):\n",
        "  system_prompt = (\"You are a helpful assistant. Your task is to correct spelling, grammar, and appropriate punctuation in the transcribed text.\")\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-4\",\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": system_prompt\n",
        "          },\n",
        "          {\n",
        "              \"role\":\"user\",\n",
        "              \"content\": transcription\n",
        "          }\n",
        "      ]\n",
        "  )\n",
        "  return response['choices'][0]['message']['content']\n",
        "\n",
        "def post_process(transcription):\n",
        "  return generate_correction(transcription)\n",
        "\n",
        "def main_function(audio, option, last_point=0):\n",
        "    chunk_transcription, updated_point = transcribe_from_point(audio, last_point)\n",
        "    \n",
        "    if option == \"Speech-to-text\":\n",
        "        return chunk_transcription, \"\", updated_point\n",
        "    elif option == \"Speech-to-text with post-processing\":\n",
        "        total_transcription = post_process(chunk_transcription)\n",
        "        return chunk_transcription, total_transcription, updated_point\n",
        "\n",
        "# Gradio component definition\n",
        "audio_input = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Speech input\")\n",
        "option_dropdown = gr.Dropdown(choices=[\"Speech-to-text\", \"Speech-to-text with post-processing\"], label=\"Options\", value=\"Speech-to-text\")\n",
        "textbox_output = gr.Textbox(label=\"Transcript\")\n",
        "textbox_processed_output = gr.Textbox(label=\"Processed transcript\")\n",
        "last_processed_point = gr.State(value=0)\n",
        "\n",
        "whisper = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=[audio_input, option_dropdown, last_processed_point],\n",
        "    outputs=[textbox_output, textbox_processed_output, last_processed_point],\n",
        "    title=\"Whisper API\",\n",
        "    description=\"This is a demo of Whisper live transcription with GPT post-processing using the OpenAI API.\",\n",
        "    allow_flagging=\"never\",\n",
        "    live=True  # the transcribe function is called repeatedly without manual input\n",
        ")\n",
        "\n",
        "whisper.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNasD+F8qfkTSVQZfNAIepg",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

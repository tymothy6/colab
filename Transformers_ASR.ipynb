{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install gradio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up the Transformers model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this code, we'll use a pre-trained ASR model from HuggingFace. By default, the automatic speech recognition model pipeline loads Facebook's `facebook/wav2vec2-base-960h` model. You can also specify a different model by passing the model name as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking algorithm, it can be used to transcribe audio samples of up to arbitrary length. Chunking is enabled by setting chunk_length_s=30 when instantiating the pipeline. With chunking enabled, the pipeline can be run with batched inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\n",
    "    \"automatic-speech-recognition\", \n",
    "    model=\"openai/whisper-large-v2\",\n",
    "    chunk_length_s=30,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a full-context ASR app with Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full-context demo means that the user speaks the full audio before using the model to run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = generator(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
    "    outputs=\"text\",\n",
    "    title=\"Automatic speech recognition with Transformers\",\n",
    "    description=\"This is a full-context demo of ASR with Transformers models.\").launch()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a streaming ASR app with DeepSpeech"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mozilla, the organization behind DeepSpeech, has moved the DeepSpeech project to a new community-driven project called Coqui. The Python package for the new project is called `stt` (speech-to-text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install deepspeech==0.8.2\n",
    "\n",
    "from deepspeech import Model\n",
    "import numpy as np\n",
    "\n",
    "model_file_path = \"deepspeech-0.8.2-models.pbmm\"\n",
    "lm_file_path = \"deepspeech-0.8.2-models.scorer\"\n",
    "beam_width = 100\n",
    "lm_alpha = 0.93\n",
    "lm_beta = 1.18\n",
    "\n",
    "model = Model(model_file_path)\n",
    "model.enableExternalScorer(lm_file_path)\n",
    "model.setScorerAlphaBeta(lm_alpha, lm_beta)\n",
    "model.setBeamWidth(beam_width)\n",
    "\n",
    "\n",
    "def reformat_freq(sr, y):\n",
    "    if sr not in (\n",
    "        48000,\n",
    "        16000,\n",
    "    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)\n",
    "        raise ValueError(\"Unsupported rate\", sr)\n",
    "    if sr == 48000:\n",
    "        y = (\n",
    "            ((y / max(np.max(y), 1)) * 32767)\n",
    "            .reshape((-1, 3))\n",
    "            .mean(axis=1)\n",
    "            .astype(\"int16\")\n",
    "        )\n",
    "        sr = 16000\n",
    "    return sr, y\n",
    "\n",
    "\n",
    "def transcribe(speech, stream):\n",
    "    _, y = reformat_freq(*speech)\n",
    "    if stream is None:\n",
    "        stream = model.createStream()\n",
    "    stream.feedAudioContent(y)\n",
    "    text = stream.intermediateDecode()\n",
    "    return text, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=[\n",
    "        gr.Audio(source=\"microphone\", type=\"numpy\"),\n",
    "        \"state\"\n",
    "    ],\n",
    "    outputs= [\n",
    "        \"text\",\n",
    "        \"state\"\n",
    "    ],\n",
    "    live=True).launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
